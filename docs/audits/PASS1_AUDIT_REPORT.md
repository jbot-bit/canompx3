# PASS 1 â€” AUDIT REPORT

**Date:** 2026-02-05
**Auditor:** Claude (MPX Code Guardian â€“ Backfill Specialist)
**Status:** AUDIT COMPLETE â€” WAITING FOR PASS 2 APPROVAL

---

## 1. REPO STATE IDENTIFICATION

| Item | Status |
|------|--------|
| Git repo | âŒ Not a git repo |
| Branch | N/A |
| Dirty/Clean | N/A |
| DATABASE_SCHEMA_SOURCE_OF_TRUTH.md | âŒ Not present |
| CLAUDE.md | âœ… Present (31KB) |
| CANONICAL_backfill_dbn_mgc_rules.txt | âœ… Present |
| CANONICAL_backfill_dbn_mgc_rules_addon.txt | âœ… Present |

**Relevant Files Found:**
- `OHLCV_MGC_FULL/ingest_dbn_mgc.py` â€” Current ingest script (UNTRUSTED)
- `pipeline/init_db.py` â€” Database initialization (created this session)
- `pipeline/paths.py` â€” Path constants (created this session)
- `pipeline/check_db.py` â€” DB inspection tool (created this session)
- `gold.db` â€” Empty DuckDB with bars_1m/bars_5m tables (created this session)
- `OHLCV_MGC_FULL/glbx-mdp3-20100912-20260203.ohlcv-1m.dbn.zst` â€” Source data (73MB compressed)

---

## 2. SOURCE DATA VERIFIED

| Property | Value |
|----------|-------|
| File | glbx-mdp3-20100912-20260203.ohlcv-1m.dbn.zst |
| Size | 73 MB compressed |
| Total records | 6,401,788 |
| Date range | 2010-09-12 to 2026-02-03 |
| Schema | ohlcv-1m âœ… |
| Timestamp format | datetime64[ns, UTC] âœ… Already UTC |
| Unique symbols | 543 (60 outrights, 483 spreads) |
| Outright pattern | `^MGC[FGHJKMNQUVXZ]\d{1,2}$` âœ… Matches all 60 |

**Chunked Iterator Verified:**
- `store.to_df(count=50000)` returns iterator âœ…
- Can process in 50K row chunks without RAM explosion âœ…

---

## 3. CURRENT SCRIPT VIOLATIONS

### Auditing: `OHLCV_MGC_FULL/ingest_dbn_mgc.py`

| Line | Violation | Rule Reference | Severity |
|------|-----------|----------------|----------|
| 253 | `df = store.to_df()` â€” Loads full 6.4M rows into RAM | CANONICAL_rules L176: "NO store.to_df() over full history" | ğŸ”´ CRITICAL |
| 281-284 | `df.apply(lambda...)` â€” Row-wise apply over millions | CANONICAL_rules L179: "NO row-wise pandas apply" | ğŸ”´ CRITICAL |
| 339 | `for _, row in front_df.iterrows()` â€” Row-wise iteration | CANONICAL_rules L179: "NO iterrows over full dataset" | ğŸ”´ CRITICAL |
| 420-447 | Builds bars_5m after ingestion | CANONICAL_rules L29: "explicitly forbidden: bars_5m" | ğŸ”´ CRITICAL |
| 159 | `max(outrights, key=...)` â€” Non-deterministic tiebreak | CANONICAL_rules L95-103: Deterministic tiebreak required | ğŸŸ¡ HIGH |
| N/A | No checkpoint system | CANONICAL_rules L128-148: Checkpoint MANDATORY | ğŸŸ¡ HIGH |
| N/A | No integrity gates after merge | CANONICAL_rules L169-173: Assert no duplicates | ğŸŸ¡ HIGH |
| N/A | No PK safety assertion before merge | CANONICAL_rules L89-93: Assert unique ts_utc | ğŸŸ¡ HIGH |
| N/A | No staging artifacts (Parquet) | ADDON L57-59: Parquet staging recommended | ğŸŸ  MEDIUM |
| N/A | Validation aborts single row, not entire backfill | CANONICAL_rules L113-116: Abort ENTIRE backfill | ğŸŸ¡ HIGH |

### Summary of Violations:
- **CRITICAL:** 4
- **HIGH:** 5
- **MEDIUM:** 1

---

## 4. COMPLIANCE CHECKLIST

### From CANONICAL_backfill_dbn_mgc_rules.txt:

| Requirement | Current Status | Action Needed |
|-------------|----------------|---------------|
| Chunked reads (no full .to_df()) | âŒ VIOLATED | Use `store.to_df(count=N)` iterator |
| Vectorized trading day calc | âŒ VIOLATED | Replace apply() with numpy.where |
| Vectorized validation | âŒ VIOLATED | Replace iterrows with boolean masks |
| No bars_5m generation | âŒ VIOLATED | Remove lines 420-447 |
| Deterministic tiebreak | âŒ VIOLATED | Add expiry parsing + lexicographic fallback |
| Checkpoint system (JSONL/SQLite) | âŒ MISSING | Implement append-only checkpoint |
| Integrity gates after merge | âŒ MISSING | Add duplicate/NULL checks |
| PK safety assertion | âŒ MISSING | Assert unique ts_utc per trading day |
| Fail-closed on invalid row | âŒ PARTIAL | Currently skips bad rows, should ABORT |
| Logging (contract per day, ties) | âŒ PARTIAL | Needs tie logging |
| Final honesty gates | âŒ MISSING | Add post-backfill verification |

### From CANONICAL_backfill_dbn_mgc_rules_addon.txt:

| Requirement | Current Status | Action Needed |
|-------------|----------------|---------------|
| Chunked iterator | âŒ VIOLATED | Already flagged above |
| Vectorized operations | âŒ VIOLATED | Already flagged above |
| Parquet staging artifacts | âŒ MISSING | Optional but recommended |
| Parallel workers â†’ artifacts | âŒ MISSING | Optional for v1 |
| Main merge step only | âŒ VIOLATED | Current script writes directly |
| Bulk loads (not row inserts) | âœ… COMPLIANT | Uses executemany |
| Idempotence via INSERT OR REPLACE | âœ… COMPLIANT | Already uses this |

---

## 5. PROPOSED ARCHITECTURE (NEW SCRIPT)

Based on both rule files, the new script should follow this pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ingest_dbn_mgc.py (v2)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. EXTRACT (chunked)                                       â”‚
â”‚     â””â”€ store.to_df(count=50000) â†’ iterator                  â”‚
â”‚                                                             â”‚
â”‚  2. TRANSFORM (vectorized, per chunk)                       â”‚
â”‚     â”œâ”€ Filter outrights (boolean mask)                      â”‚
â”‚     â”œâ”€ Calculate trading_day (numpy.where)                  â”‚
â”‚     â”œâ”€ Validate OHLCV (vectorized boolean masks)            â”‚
â”‚     â””â”€ If ANY invalid â†’ ABORT ENTIRE BACKFILL               â”‚
â”‚                                                             â”‚
â”‚  3. AGGREGATE (per trading day)                             â”‚
â”‚     â”œâ”€ Sum volume per contract                              â”‚
â”‚     â”œâ”€ Select front-month (deterministic tiebreak)          â”‚
â”‚     â””â”€ Assert unique ts_utc (PK safety)                     â”‚
â”‚                                                             â”‚
â”‚  4. STAGE (optional but recommended)                        â”‚
â”‚     â””â”€ Write chunk to temp Parquet artifact                 â”‚
â”‚                                                             â”‚
â”‚  5. MERGE (per chunk, transactional)                        â”‚
â”‚     â”œâ”€ BEGIN                                                â”‚
â”‚     â”œâ”€ INSERT OR REPLACE INTO bars_1m                       â”‚
â”‚     â”œâ”€ Integrity gates (no duplicates, no NULL)             â”‚
â”‚     â”œâ”€ COMMIT (or ROLLBACK on failure)                      â”‚
â”‚     â””â”€ Update checkpoint (append-only JSONL)                â”‚
â”‚                                                             â”‚
â”‚  6. FINAL GATES                                             â”‚
â”‚     â”œâ”€ Verify ts_utc type = TIMESTAMPTZ                     â”‚
â”‚     â”œâ”€ Verify no duplicate (symbol, ts_utc)                 â”‚
â”‚     â”œâ”€ Verify no NULL source_symbol                         â”‚
â”‚     â””â”€ Exit 0 (success) or non-zero (failure)               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. FILE CHANGES REQUIRED

### DELETE:
- None (keep old script for reference in oldscripts/)

### MOVE:
- `OHLCV_MGC_FULL/ingest_dbn_mgc.py` â†’ `oldscripts/ARCHIVE_ingest_dbn_mgc_v1.py`

### CREATE:
| File | Purpose |
|------|---------|
| `pipeline/ingest_dbn_mgc.py` | New compliant ingest script |
| `pipeline/checkpoints/` | Directory for checkpoint JSONL files |
| `pipeline/staging/` | Directory for temp Parquet artifacts (optional) |

### MODIFY:
- `pipeline/init_db.py` â€” Ensure bars_1m schema matches exactly (already correct)

---

## 7. CLI INTERFACE (PROPOSED)

```bash
# Full backfill (will take time)
python pipeline/ingest_dbn_mgc.py

# Date range backfill
python pipeline/ingest_dbn_mgc.py --start 2020-01-01 --end 2025-12-31

# Resume from checkpoint
python pipeline/ingest_dbn_mgc.py --resume

# Retry failed chunks
python pipeline/ingest_dbn_mgc.py --retry-failed

# Dry run (validate only)
python pipeline/ingest_dbn_mgc.py --dry-run

# Configure chunk size (trading days per commit)
python pipeline/ingest_dbn_mgc.py --chunk-days 7

# Configure row batch size for DBN reading
python pipeline/ingest_dbn_mgc.py --batch-size 50000
```

---

## 8. RISK ASSESSMENT

| Risk | Mitigation |
|------|------------|
| RAM explosion | Chunked iterator (50K rows max in memory) |
| Data corruption | Transactional commits per chunk + rollback |
| Non-deterministic results | Deterministic tiebreak + stable sort |
| Silent bad data | Fail-closed validation â†’ abort on ANY invalid row |
| Lost progress | Checkpoint system â†’ resume from last done chunk |
| Duplicate rows | PK safety + integrity gates |

---

## 9. PASS 1 CONCLUSION

**Current script `OHLCV_MGC_FULL/ingest_dbn_mgc.py` is NOT COMPLIANT.**

It violates 4 CRITICAL and 5 HIGH severity rules from the canonical spec.

**Running this script will:**
- Potentially crash from RAM exhaustion (6.4M rows Ã— pandas overhead)
- Take hours due to row-wise operations
- Generate forbidden bars_5m table
- Produce non-deterministic results on contract ties
- Have no checkpoint/resume capability
- Skip bad rows instead of failing closed

---

## 10. NEXT STEP

**Awaiting user approval to proceed to PASS 2 (BUILD).**

When approved, I will:
1. Archive the current script
2. Create a new compliant `pipeline/ingest_dbn_mgc.py`
3. Implement checkpoint system
4. Test with `--dry-run` first
5. Run full backfill

---

**END OF PASS 1 AUDIT REPORT**
